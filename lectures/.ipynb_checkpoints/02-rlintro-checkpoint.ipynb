{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as st\n",
    "import scipy.integrate as integrate\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn import linear_model\n",
    "from sklearn.utils.testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import statsmodels.api as sm\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"colorblind\")\n",
    "palette = sns.color_palette()\n",
    "figsize = (15,8)\n",
    "legend_fontsize = 16\n",
    "\n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'sans-serif'})\n",
    "rc('text', usetex=True)\n",
    "rc('text.latex',preamble=r'\\usepackage[utf8]{inputenc}')\n",
    "rc('text.latex',preamble=r'\\usepackage[russian]{babel}')\n",
    "rc('figure', **{'dpi': 300})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from gym import envs\n",
    "print(\"\\n\".join([\"%s\" % x for x in envs.registry.all()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "for _ in range(5):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.env.P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy iteration по уравнениям Беллмана"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nS, nA = env.env.nS, env.env.nA\n",
    "final_states = np.where([ len(env.env.P[x][0]) == 1 and env.env.P[x][0][0][3] == True for x in env.env.P.keys() ])[0]\n",
    "\n",
    "def get_random_V(env):\n",
    "    V = np.random.random(nS)\n",
    "    V[final_states] = 0.0\n",
    "    return V\n",
    "\n",
    "def get_random_Q(env):\n",
    "    Q = np.random.random(size=(nS, nA))\n",
    "    Q[final_states, :] = 0.0\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_V_by_policy(env, pi, gamma=1.0):\n",
    "    V = get_random_V(env)\n",
    "    while True:\n",
    "        new_V = np.array([ \\\n",
    "          np.sum([ x[0] * ( x[2] + gamma * V[x[1]] ) for x in env.env.P[cur_state][pi[cur_state]] ]) \\\n",
    "        for cur_state in range(nS) ])\n",
    "        if np.sum((V - new_V) ** 2) < 0.001:\n",
    "            break\n",
    "        V = new_V\n",
    "    return V\n",
    "\n",
    "def compute_policy_by_V(env, V, gamma=1.0):\n",
    "    return np.argmax( np.array([[ \\\n",
    "          np.sum([ x[0] * ( x[2] + gamma * V[x[1]] ) for x in env.env.P[s][a] ]) \\\n",
    "        for a in range(nA) ] for s in range(nS)]), axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_V_and_pi(env, gamma=1.0):\n",
    "    V = get_random_V(env)\n",
    "    pi = np.random.randint(nA, size=nS)\n",
    "\n",
    "    while True:\n",
    "        V = compute_V_by_policy(env, pi, gamma)\n",
    "        new_pi = compute_policy_by_V(env, V, gamma)\n",
    "        if np.array_equal(pi, new_pi):\n",
    "            break\n",
    "        pi = new_pi\n",
    "    \n",
    "    return V, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env._max_episode_steps = 10000\n",
    "\n",
    "num_experiments = 200\n",
    "num_steps, total_reward = [], []\n",
    "\n",
    "V, pi = compute_V_and_pi(env)\n",
    "\n",
    "for _ in range(num_experiments):\n",
    "    env.reset()\n",
    "    total_reward.append(0)\n",
    "    for step in range(1000):\n",
    "#         env.render()\n",
    "#         print(\"Стратегия выбирает %s\" % pi[env.env.s])\n",
    "        observation, reward, done, info = env.step(pi[env.env.s])\n",
    "        total_reward[-1] += reward\n",
    "        if done:\n",
    "            num_steps.append(step+1)\n",
    "            print(\"Эпизод закончился за %d шагов в состоянии %s, общая награда %d\" % (num_steps[-1], env.env.s, total_reward[-1]) )\n",
    "            break\n",
    "env.close()\n",
    "\n",
    "print(\"\\nСредняя награда: %.6f\\nСреднее число шагов: %.6f\" % (np.mean(total_reward), np.mean(num_steps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_experiments_pi(env, pi, num_experiments=1000):\n",
    "    num_steps, total_reward = [], []\n",
    "    for _ in range(num_experiments):\n",
    "        env.reset()\n",
    "        num_steps.append(0)\n",
    "        total_reward.append(0)\n",
    "        for _ in range(1000):\n",
    "            observation, reward, done, info = env.step(pi[env.env.s])\n",
    "            total_reward[-1] += reward\n",
    "            num_steps[-1] += 1\n",
    "            if done:\n",
    "                break\n",
    "    env.close()\n",
    "    return np.mean(total_reward), np.mean(num_steps)\n",
    "\n",
    "def conduct_experiments(env, gamma=1.0, num_experiments=100, num_experiments_pi=10):\n",
    "    num_steps, total_reward = [], []\n",
    "    for _ in range(num_experiments):\n",
    "        V, pi = compute_V_and_pi(env, gamma=gamma)\n",
    "        cur_steps, cur_reward = conduct_experiments_pi(env, pi, num_experiments=num_experiments_pi)\n",
    "        num_steps.append(cur_steps)\n",
    "        total_reward.append(cur_reward)\n",
    "    return np.mean(num_steps), np.mean(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env._max_episode_steps = 10000\n",
    "results = []\n",
    "for gamma in np.linspace(0.5, 1.0, 10):\n",
    "    mean_reward, mean_steps = conduct_experiments(env, gamma, num_experiments=100, num_experiments_pi=10)\n",
    "    results.append([gamma, mean_reward, mean_steps])\n",
    "    print(\"gamma=%.4f, mean reward = %.4f, mean steps = %.4f\" % (gamma, mean_reward, mean_steps) )\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results):\n",
    "    gammas, rewards, numsteps = [x[0] for x in results], [x[1] for x in results], [x[2] for x in results]\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.grid(None)\n",
    "\n",
    "    line1 = ax.plot(gammas, rewards, label=\"Средние награды\", color=\"C0\")\n",
    "    line2 = ax2.plot(gammas, numsteps, label=\"Среднее число шагов\", color=\"C1\")\n",
    "\n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax.legend(lines, labels, loc=2)\n",
    "    ax.set_xlim((0.5, 1.0))\n",
    "    # ax.set_ylim((0.1, 0.8))\n",
    "    # ax2.set_ylim((10, 45))\n",
    "    return fig, ax\n",
    "\n",
    "fig, ax = plot_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value iteration по уравнениям Беллмана"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_V_max(env, gamma=1.0):\n",
    "    V = get_random_V(env)\n",
    "    while True:\n",
    "        new_V = np.array([ [ \\\n",
    "          np.sum([ x[0] * ( x[2] + gamma * V[x[1]] ) for x in env.env.P[cur_state][cur_action] ]) \\\n",
    "        for cur_action in range(nA) ] for cur_state in range(nS) ])\n",
    "        new_V = np.max(new_V, axis=1)\n",
    "        if np.sum((V - new_V) ** 2) < 0.001:\n",
    "            break\n",
    "        V = new_V\n",
    "    return V\n",
    "\n",
    "def compute_Q_max(env, gamma=1.0):\n",
    "    Q = get_random_Q(env)\n",
    "    while True:\n",
    "        new_Q = np.array([ [ \\\n",
    "          np.sum([ x[0] * ( x[2] + gamma * np.max(Q[x[1], :]) ) for x in env.env.P[cur_state][cur_action] ]) \\\n",
    "        for cur_action in range(nA) ] for cur_state in range(nS) ])\n",
    "#         new_V = np.max(new_V, axis=1)\n",
    "        if np.sum((Q - new_Q) ** 2) < 0.001:\n",
    "            break\n",
    "        Q = new_Q\n",
    "    return Q\n",
    "\n",
    "def compute_policy_by_Q(env, Q, gamma=1.0):\n",
    "    return np.argmax( Q, axis=1 )\n",
    "\n",
    "def conduct_experiments_max(env, gamma, use_Q=False, num_experiments=100, num_experiments_pi=200):\n",
    "    num_steps, total_reward = [], []\n",
    "    for _ in range(num_experiments):\n",
    "        if use_Q:\n",
    "            Q = compute_Q_max(env, gamma=gamma)\n",
    "            pi = compute_policy_by_Q(env, Q)\n",
    "        else:\n",
    "            V = compute_V_max(env, gamma=gamma)\n",
    "            pi = compute_policy_by_V(env, V)\n",
    "        result = conduct_experiments_pi(env, pi, num_experiments=num_experiments_pi)\n",
    "        num_steps.append(result[0])\n",
    "        total_reward.append(result[1])\n",
    "    return np.mean(num_steps), np.mean(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env._max_episode_steps = 10000\n",
    "\n",
    "V = compute_V_max(env)\n",
    "pi = compute_policy_by_V(env, V, gamma=0.99)\n",
    "print(pi)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env._max_episode_steps = 10000\n",
    "\n",
    "Q = compute_Q_max(env)\n",
    "pi = compute_policy_by_Q(env, Q)\n",
    "print(pi)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env._max_episode_steps = 10000\n",
    "\n",
    "results_max = []\n",
    "for gamma in np.linspace(0.5, 1.0, 20):\n",
    "    mean_reward, mean_steps = conduct_experiments_max(env, gamma, use_Q=True, num_experiments=20, num_experiments_pi=100)\n",
    "    results_max.append([gamma, mean_reward, mean_steps])\n",
    "    print(\"gamma=%.4f, mean reward = %.4f, mean steps = %.4f\" % (gamma, mean_reward, mean_steps) )\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_results(results_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First-visit Monte Carlo по состояниям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, pi):\n",
    "    env.reset()\n",
    "    states, rewards = [env.env.s], [0]\n",
    "    for _ in range(1000):\n",
    "        observation, reward, done, info = env.step(pi[env.env.s])\n",
    "        states.append(env.env.s)\n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "    return states, rewards\n",
    "\n",
    "def update_returns(R, states, rewards, R_all=None):\n",
    "    state_first_visit = [-1 for _ in range(nS)]\n",
    "    for t, state in enumerate(states):\n",
    "        if state_first_visit[state] == -1:\n",
    "            state_first_visit[state] = t\n",
    "    g = 0\n",
    "    if state_first_visit[states[-1]] == len(states)-1:\n",
    "        R[states[-1]].append(g)\n",
    "        if R_all is not None:\n",
    "            R_all[states[t]].append(g)\n",
    "    for t in range(len(states)-2, -1, -1):\n",
    "        g =  g * gamma + rewards[t+1]\n",
    "        if state_first_visit[states[t]] == t:\n",
    "            R[states[t]].append(g)\n",
    "            if R_all is not None:\n",
    "                R_all[states[t]].append(g)\n",
    "    for state in range(nS):\n",
    "        if state_first_visit[state] == -1:\n",
    "            R_all[state].append(np.nan)\n",
    "    return R, R_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env._max_episode_steps = 10000\n",
    "total_episodes = 1000\n",
    "gamma = 0.9\n",
    "\n",
    "Q = compute_Q_max(env, gamma)\n",
    "pi = compute_policy_by_Q(env, Q)\n",
    "\n",
    "V = get_random_V(env)\n",
    "R, R_all = [ [] for _ in range(nS) ], [ [] for _ in range(nS) ]\n",
    "\n",
    "for _ in range(total_episodes):\n",
    "    states, rewards = run_episode(env, pi)\n",
    "    R, R_all = update_returns(R, states, rewards, R_all=R_all)\n",
    "\n",
    "for state in range(nS):\n",
    "    if len(R[state]) > 0:\n",
    "        V[state] = np.mean(R[state])\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_returns = [ np.true_divide( np.nancumsum(R_all[s]), np.arange(1, len(R_all[s])+1) ) for s in range(nS) ]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "for i,s in enumerate([0, 1, 9, 14]):\n",
    "    ax.plot(np.arange(1, len(R_all[s])+1), mean_returns[s], label=\"Состояние %d\" % s, color=\"C%d\" % i)\n",
    "\n",
    "ax.legend(loc=\"center right\")\n",
    "ax.set_xlim((1, 1000))\n",
    "# ax.set_ylim((0.1, 0.8))\n",
    "# ax2.set_ylim((10, 45))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode_actions(env, pi, eps=0.0):\n",
    "    env.reset()\n",
    "    next_action = pi[env.env.s] if np.random.rand() > eps else np.random.randint(nA)\n",
    "    states, actions, rewards = [env.env.s], [next_action], [0]\n",
    "    for _ in range(1000):\n",
    "        observation, reward, done, info = env.step(next_action)\n",
    "        states.append(env.env.s)\n",
    "        next_action = pi[env.env.s] if np.random.rand() > eps else np.random.randint(nA)\n",
    "        actions.append(next_action)\n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "    return states, actions, rewards\n",
    "\n",
    "def update_returns_actions(R, states, actions, rewards, gamma=1.0, R_all=None):\n",
    "    state_first_visit = [-1 for _ in range(nS)]\n",
    "    for t, state in enumerate(states):\n",
    "        if state_first_visit[state] == -1:\n",
    "            state_first_visit[state] = t\n",
    "    g = 0\n",
    "    if R_all is not None:\n",
    "        _ = [R_all[s][a].append(np.nan) for a in range(nA) for s in range(nS)]\n",
    "    if state_first_visit[states[-1]] == len(states)-1:\n",
    "        R[states[-1]][actions[-1]].append(g)\n",
    "        if R_all is not None:\n",
    "            R_all[states[t]][actions[t]][-1] = g\n",
    "    for t in range(len(states)-2, -1, -1):\n",
    "        g =  g * gamma + rewards[t+1]\n",
    "        if state_first_visit[states[t]] == t:\n",
    "            R[states[t]][actions[t]].append(g)\n",
    "            if R_all is not None:\n",
    "                R_all[states[t]][actions[t]][-1] = g\n",
    "    return R, R_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env._max_episode_steps = 10000\n",
    "total_episodes = 20000\n",
    "gamma = 0.9\n",
    "\n",
    "Q_max = compute_Q_max(env, gamma)\n",
    "pi = compute_policy_by_Q(env, Q_max)\n",
    "\n",
    "R, R_all = [ [ [] for _ in range(nA) ] for _ in range(nS) ], [ [ [] for _ in range(nA) ] for _ in range(nS) ]\n",
    "\n",
    "for _ in range(total_episodes):\n",
    "    states, actions, rewards = run_episode_actions(env, pi, eps=0.)\n",
    "    R, R_all = update_returns_actions(R, states, actions, rewards, gamma=gamma, R_all=R_all)\n",
    "\n",
    "Q = np.zeros((nS, nA))\n",
    "for s in range(nS):\n",
    "    for a in range(nA):\n",
    "        if len(R[s][a]) > 0:\n",
    "            Q[s][a] = np.mean(R[s][a])\n",
    "        else:\n",
    "            Q[s][a] = -1\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_returns = [[[] for _ in range(nA)] for _ in range(nS)]\n",
    "for s in range(nS):\n",
    "    for a in range(nA):\n",
    "        cur_denominator, cur_value = 0., 0.\n",
    "        for r in R_all[s][a]:\n",
    "            if not np.isnan(r):\n",
    "                cur_denominator += 1.\n",
    "                cur_value += r\n",
    "            mean_returns[s][a].append(cur_value / cur_denominator if cur_denominator > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "ss = 14\n",
    "# for i,s_a in enumerate([(ss,0), (ss,1), (ss,2), (ss,3)]):\n",
    "for i,s_a in enumerate([(0,0), (4,0), (14,1), (13,2)]):\n",
    "    s,a = s_a\n",
    "    ax.plot(np.arange(1, len(R_all[s][a])+1), mean_returns[s][a], label=\"Состояние %d, действие %d\" % (s, a), color=\"C%d\" % i)\n",
    "\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.set_xlim((1, 2000))\n",
    "# ax.set_ylim((0.1, 0.8))\n",
    "# ax2.set_ylim((10, 45))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
